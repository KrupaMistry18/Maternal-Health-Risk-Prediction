# -*- coding: utf-8 -*-
"""MaternalHealth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l1WYTwWrxLDcXX8Ne1IjeNA4cZHdWLmB
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
data = pd.read_csv('Maternal_Health_Risk_Assessment Original Dataset.csv')

# Remove outliers in SystolicBP
Q1 = data['SystolicBP'].quantile(0.25)
Q3 = data['SystolicBP'].quantile(0.75)
IQR = Q3 - Q1
data = data[(data['SystolicBP'] >= (Q1 - 1.5 * IQR)) & (data['SystolicBP'] <= (Q3 + 1.5 * IQR))]

# Create BP_Status
def categorize_bp(systolic, diastolic):
    if systolic < 120 and diastolic < 80:
        return 'Normal'
    elif 120 <= systolic <= 129 and diastolic < 80:
        return 'Elevated'
    elif systolic >= 130 or diastolic >= 80:
        return 'High'
    return 'Unknown'

data['BP_Status'] = data.apply(lambda row: categorize_bp(row['SystolicBP'], row['DiastolicBP']), axis=1)
data['BP_Status_encoded'] = data['BP_Status'].map({'High': 1, 'Normal': 0, 'Elevated': 0.5})

# Encode target
label_encoder = LabelEncoder()
data['RiskLevel'] = label_encoder.fit_transform(data['RiskLevel'])

# Scale features
scaler = StandardScaler()
data[['SystolicBP', 'DiastolicBP', 'Blood glucose', 'BodyTemp', 'HeartRate']] = scaler.fit_transform(
    data[['SystolicBP', 'DiastolicBP', 'Blood glucose', 'BodyTemp', 'HeartRate']]
)

# Train-test split
X = data.drop('RiskLevel', axis=1)
y = data['RiskLevel']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Drop non-numeric columns (like 'BP_Status') before training
X = data.drop(['RiskLevel', 'BP_Status'], axis=1)  # 'BP_Status_encoded' is numeric and can stay
y = data['RiskLevel']

# Split the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# Classification report
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

!pip install -q ucimlrepo scikit-learn matplotlib joblib

import numpy as np, pandas as pd
from ucimlrepo import fetch_ucirepo
from pathlib import Path
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, f1_score, classification_report,
                             confusion_matrix, ConfusionMatrixDisplay)
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import joblib

rng = np.random.RandomState(42)

dataset = fetch_ucirepo(id=863)  # Maternal Health Risk (UCI dataset ID)
X = dataset.data.features.copy()
y = dataset.data.targets.copy()
df = pd.concat([X, y], axis=1)

df = df.drop_duplicates().copy()

Q1, Q3 = df["SystolicBP"].quantile([0.25, 0.75]); IQR = Q3 - Q1
df = df[(df["SystolicBP"] >= (Q1 - 1.5*IQR)) & (df["SystolicBP"] <= (Q3 + 1.5*IQR))].copy()

def categorize_bp(sys, dia):
    if sys < 120 and dia < 80: return "Normal"
    if 120 <= sys <= 129 and dia < 80: return "Elevated"
    if sys >= 130 or dia >= 80: return "High"
    return "Unknown"

df["BP_Status"] = df.apply(lambda r: categorize_bp(r["SystolicBP"], r["DiastolicBP"]), axis=1)
df["BP_Status_encoded"] = df["BP_Status"].map({"Normal":0, "Elevated":0.5, "High":1}).fillna(0).astype(float)

y = df["RiskLevel"].astype("category")
class_names = y.cat.categories.tolist()
y = y.cat.codes  # int labels

X = df.drop(columns=["RiskLevel", "BP_Status"])

X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42
)

num_cols = X.columns.tolist()
pre = ColumnTransformer([("num", StandardScaler(), num_cols)], remainder="drop")

baseline = Pipeline([
    ("pre", pre),
    ("clf", RandomForestClassifier(
        n_estimators=300, random_state=42, n_jobs=-1, class_weight="balanced"
    ))
])
baseline.fit(X_train, y_train)

def evaluate(split_name, Xs, ys, model):
    yp = model.predict(Xs)
    acc = accuracy_score(ys, yp)
    f1m = f1_score(ys, yp, average="macro")
    print(f"{split_name}  Acc: {acc:.3f} | Macro-F1: {f1m:.3f}")
    print(classification_report(ys, yp, target_names=class_names))
    cm = confusion_matrix(ys, yp)
    ConfusionMatrixDisplay(cm, display_labels=class_names).plot(cmap="Blues")
    plt.title(f"Confusion Matrix - {split_name}")
    plt.show()

print("=== Baseline ===")
evaluate("Validation", X_val, y_val, baseline)

param_dist = {
    "clf__n_estimators": [200, 300, 500, 800],
    "clf__max_depth": [None, 6, 10, 14, 18],
    "clf__min_samples_split": [2, 5, 10],
    "clf__min_samples_leaf": [1, 2, 4],
    "clf__max_features": ["sqrt", "log2", None],
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
search = RandomizedSearchCV(
    estimator=baseline, param_distributions=param_dist,
    n_iter=25, scoring="f1_macro", n_jobs=-1, cv=cv,
    random_state=42, verbose=1
)
search.fit(X_trainval, y_trainval)
print("Best F1_macro:", search.best_score_)
print("Best params:", search.best_params_)

best_model = search.best_estimator_

print("=== Test Performance (Tuned) ===")
evaluate("Test", X_test, y_test, best_model)

r = permutation_importance(best_model, X_test, y_test, scoring="f1_macro",
                           n_repeats=10, random_state=42, n_jobs=-1)
imp = pd.DataFrame({"feature": num_cols, "importance_mean": r.importances_mean}) \
        .sort_values("importance_mean", ascending=False)
print("Top features by permutation importance:")
print(imp.head(10))

Path("artifacts").mkdir(exist_ok=True)
joblib.dump(best_model, "artifacts/maternal_health_rf.joblib")
imp.to_csv("artifacts/feature_importance.csv", index=False)
print("Saved -> artifacts/maternal_health_rf.joblib  &  artifacts/feature_importance.csv")